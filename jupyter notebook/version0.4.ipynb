{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7748856-312f-4b93-aa05-13f72cba6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import idx2numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "\n",
    "def load_dataset2():\n",
    "    # 数据预处理\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # 加载完整的 MNIST 训练数据集\n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 随机选择 20,000 个样本\n",
    "    subset_indices = torch.randperm(len(full_dataset))[:20000]\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "    # 将 20,000 个样本分为 5,000 个训练集、5,000 个验证集和 10,000 个测试集\n",
    "    train_set, val_set, test_set = random_split(subset_dataset, [5000, 5000, 10000])\n",
    "\n",
    "    # 打乱训练集中的 2,500 个样本的标签\n",
    "    rand_indices = torch.randperm(len(train_set))[:2500]\n",
    "    for idx in rand_indices:\n",
    "        # 随机生成一个新的标签\n",
    "        new_label = torch.randint(0, 10, (1,)).item()\n",
    "        train_set.dataset.dataset.targets[subset_indices[train_set.indices[idx]]] = new_label\n",
    "\n",
    "    # 创建数据加载器\n",
    "    trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "def test(net, testloader):\n",
    "    # 测试网络\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
    "\n",
    "\n",
    "trainloader, valloader, testloader = load_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1199e6b0-07f1-42bd-a2a6-3f787c1435ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net1 = SimpleNet()\n",
    "\n",
    "N = 5000\n",
    "la1 = torch.rand([N,1],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f882bed-8e75-4ec4-87ba-176e49c9ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 46.89 %\n",
      "[Epoch 1, Batch 1] loss: 0.009\n",
      "[Epoch 1, Batch 2] loss: 0.011\n",
      "[Epoch 1, Batch 3] loss: 0.025\n",
      "[Epoch 1, Batch 4] loss: 0.026\n",
      "[Epoch 1, Batch 5] loss: 0.044\n",
      "[Epoch 1, Batch 6] loss: 0.053\n",
      "[Epoch 1, Batch 7] loss: 0.072\n",
      "[Epoch 1, Batch 8] loss: 0.085\n",
      "[Epoch 1, Batch 9] loss: 0.087\n",
      "[Epoch 1, Batch 10] loss: 0.101\n",
      "[Epoch 1, Batch 11] loss: 0.101\n",
      "[Epoch 1, Batch 12] loss: 0.109\n",
      "[Epoch 1, Batch 13] loss: 0.113\n",
      "[Epoch 1, Batch 14] loss: 0.116\n",
      "[Epoch 1, Batch 15] loss: 0.129\n",
      "[Epoch 1, Batch 16] loss: 0.146\n",
      "[Epoch 1, Batch 17] loss: 0.153\n",
      "[Epoch 1, Batch 18] loss: 0.157\n",
      "[Epoch 1, Batch 19] loss: 0.164\n",
      "[Epoch 1, Batch 20] loss: 0.172\n",
      "[Epoch 1, Batch 21] loss: 0.173\n",
      "[Epoch 1, Batch 22] loss: 0.181\n",
      "[Epoch 1, Batch 23] loss: 0.183\n",
      "[Epoch 1, Batch 24] loss: 0.188\n",
      "[Epoch 1, Batch 25] loss: 0.201\n",
      "[Epoch 1, Batch 26] loss: 0.210\n",
      "[Epoch 1, Batch 27] loss: 0.220\n",
      "[Epoch 1, Batch 28] loss: 0.226\n",
      "[Epoch 1, Batch 29] loss: 0.237\n",
      "[Epoch 1, Batch 30] loss: 0.248\n",
      "[Epoch 1, Batch 31] loss: 0.257\n",
      "[Epoch 1, Batch 32] loss: 0.266\n",
      "[Epoch 1, Batch 33] loss: 0.269\n",
      "[Epoch 1, Batch 34] loss: 0.285\n",
      "[Epoch 1, Batch 35] loss: 0.288\n",
      "[Epoch 1, Batch 36] loss: 0.289\n",
      "[Epoch 1, Batch 37] loss: 0.295\n",
      "[Epoch 1, Batch 38] loss: 0.305\n",
      "[Epoch 1, Batch 39] loss: 0.315\n",
      "[Epoch 1, Batch 40] loss: 0.317\n",
      "[Epoch 1, Batch 41] loss: 0.317\n",
      "[Epoch 1, Batch 42] loss: 0.317\n",
      "[Epoch 1, Batch 43] loss: 0.329\n",
      "[Epoch 1, Batch 44] loss: 0.336\n",
      "[Epoch 1, Batch 45] loss: 0.351\n",
      "[Epoch 1, Batch 46] loss: 0.358\n",
      "[Epoch 1, Batch 47] loss: 0.366\n",
      "[Epoch 1, Batch 48] loss: 0.368\n",
      "[Epoch 1, Batch 49] loss: 0.374\n",
      "[Epoch 1, Batch 50] loss: 0.375\n",
      "[Epoch 1, Batch 51] loss: 0.380\n",
      "[Epoch 1, Batch 52] loss: 0.395\n",
      "[Epoch 1, Batch 53] loss: 0.405\n",
      "[Epoch 1, Batch 54] loss: 0.415\n",
      "[Epoch 1, Batch 55] loss: 0.418\n",
      "[Epoch 1, Batch 56] loss: 0.431\n",
      "[Epoch 1, Batch 57] loss: 0.445\n",
      "[Epoch 1, Batch 58] loss: 0.451\n",
      "[Epoch 1, Batch 59] loss: 0.465\n",
      "[Epoch 1, Batch 60] loss: 0.468\n",
      "[Epoch 1, Batch 61] loss: 0.482\n",
      "[Epoch 1, Batch 62] loss: 0.485\n",
      "[Epoch 1, Batch 63] loss: 0.490\n",
      "[Epoch 1, Batch 64] loss: 0.495\n",
      "[Epoch 1, Batch 65] loss: 0.508\n",
      "[Epoch 1, Batch 66] loss: 0.511\n",
      "[Epoch 1, Batch 67] loss: 0.519\n",
      "[Epoch 1, Batch 68] loss: 0.522\n",
      "[Epoch 1, Batch 69] loss: 0.525\n",
      "[Epoch 1, Batch 70] loss: 0.526\n",
      "[Epoch 1, Batch 71] loss: 0.532\n",
      "[Epoch 1, Batch 72] loss: 0.536\n",
      "[Epoch 1, Batch 73] loss: 0.536\n",
      "[Epoch 1, Batch 74] loss: 0.537\n",
      "[Epoch 1, Batch 75] loss: 0.540\n",
      "[Epoch 1, Batch 76] loss: 0.544\n",
      "[Epoch 1, Batch 77] loss: 0.550\n",
      "[Epoch 1, Batch 78] loss: 0.550\n",
      "[Epoch 1, Batch 79] loss: 0.560\n",
      "[Epoch 2, Batch 1] loss: 0.007\n",
      "[Epoch 2, Batch 2] loss: 0.008\n",
      "[Epoch 2, Batch 3] loss: 0.014\n",
      "[Epoch 2, Batch 4] loss: 0.015\n",
      "[Epoch 2, Batch 5] loss: 0.021\n",
      "[Epoch 2, Batch 6] loss: 0.025\n",
      "[Epoch 2, Batch 7] loss: 0.036\n",
      "[Epoch 2, Batch 8] loss: 0.043\n",
      "[Epoch 2, Batch 9] loss: 0.046\n",
      "[Epoch 2, Batch 10] loss: 0.054\n",
      "[Epoch 2, Batch 11] loss: 0.054\n",
      "[Epoch 2, Batch 12] loss: 0.058\n",
      "[Epoch 2, Batch 13] loss: 0.061\n",
      "[Epoch 2, Batch 14] loss: 0.063\n",
      "[Epoch 2, Batch 15] loss: 0.073\n",
      "[Epoch 2, Batch 16] loss: 0.083\n",
      "[Epoch 2, Batch 17] loss: 0.088\n",
      "[Epoch 2, Batch 18] loss: 0.092\n",
      "[Epoch 2, Batch 19] loss: 0.100\n",
      "[Epoch 2, Batch 20] loss: 0.108\n",
      "[Epoch 2, Batch 21] loss: 0.109\n",
      "[Epoch 2, Batch 22] loss: 0.117\n",
      "[Epoch 2, Batch 23] loss: 0.118\n",
      "[Epoch 2, Batch 24] loss: 0.126\n",
      "[Epoch 2, Batch 25] loss: 0.140\n",
      "[Epoch 2, Batch 26] loss: 0.146\n",
      "[Epoch 2, Batch 27] loss: 0.158\n",
      "[Epoch 2, Batch 28] loss: 0.163\n",
      "[Epoch 2, Batch 29] loss: 0.170\n",
      "[Epoch 2, Batch 30] loss: 0.179\n",
      "[Epoch 2, Batch 31] loss: 0.188\n",
      "[Epoch 2, Batch 32] loss: 0.196\n",
      "[Epoch 2, Batch 33] loss: 0.198\n",
      "[Epoch 2, Batch 34] loss: 0.210\n",
      "[Epoch 2, Batch 35] loss: 0.213\n",
      "[Epoch 2, Batch 36] loss: 0.214\n",
      "[Epoch 2, Batch 37] loss: 0.219\n",
      "[Epoch 2, Batch 38] loss: 0.226\n",
      "[Epoch 2, Batch 39] loss: 0.233\n",
      "[Epoch 2, Batch 40] loss: 0.234\n",
      "[Epoch 2, Batch 41] loss: 0.235\n",
      "[Epoch 2, Batch 42] loss: 0.235\n",
      "[Epoch 2, Batch 43] loss: 0.244\n",
      "[Epoch 2, Batch 44] loss: 0.251\n",
      "[Epoch 2, Batch 45] loss: 0.263\n",
      "[Epoch 2, Batch 46] loss: 0.269\n",
      "[Epoch 2, Batch 47] loss: 0.276\n",
      "[Epoch 2, Batch 48] loss: 0.277\n",
      "[Epoch 2, Batch 49] loss: 0.284\n",
      "[Epoch 2, Batch 50] loss: 0.285\n",
      "[Epoch 2, Batch 51] loss: 0.289\n",
      "[Epoch 2, Batch 52] loss: 0.301\n",
      "[Epoch 2, Batch 53] loss: 0.314\n",
      "[Epoch 2, Batch 54] loss: 0.325\n",
      "[Epoch 2, Batch 55] loss: 0.327\n",
      "[Epoch 2, Batch 56] loss: 0.340\n",
      "[Epoch 2, Batch 57] loss: 0.352\n",
      "[Epoch 2, Batch 58] loss: 0.358\n",
      "[Epoch 2, Batch 59] loss: 0.370\n",
      "[Epoch 2, Batch 60] loss: 0.372\n",
      "[Epoch 2, Batch 61] loss: 0.380\n",
      "[Epoch 2, Batch 62] loss: 0.383\n",
      "[Epoch 2, Batch 63] loss: 0.388\n",
      "[Epoch 2, Batch 64] loss: 0.393\n",
      "[Epoch 2, Batch 65] loss: 0.407\n",
      "[Epoch 2, Batch 66] loss: 0.410\n",
      "[Epoch 2, Batch 67] loss: 0.418\n",
      "[Epoch 2, Batch 68] loss: 0.421\n",
      "[Epoch 2, Batch 69] loss: 0.423\n",
      "[Epoch 2, Batch 70] loss: 0.425\n",
      "[Epoch 2, Batch 71] loss: 0.431\n",
      "[Epoch 2, Batch 72] loss: 0.437\n",
      "[Epoch 2, Batch 73] loss: 0.437\n",
      "[Epoch 2, Batch 74] loss: 0.438\n",
      "[Epoch 2, Batch 75] loss: 0.443\n",
      "[Epoch 2, Batch 76] loss: 0.448\n",
      "[Epoch 2, Batch 77] loss: 0.455\n",
      "[Epoch 2, Batch 78] loss: 0.456\n",
      "[Epoch 2, Batch 79] loss: 0.472\n",
      "[Epoch 3, Batch 1] loss: 0.008\n",
      "[Epoch 3, Batch 2] loss: 0.009\n",
      "[Epoch 3, Batch 3] loss: 0.016\n",
      "[Epoch 3, Batch 4] loss: 0.016\n",
      "[Epoch 3, Batch 5] loss: 0.025\n",
      "[Epoch 3, Batch 6] loss: 0.032\n",
      "[Epoch 3, Batch 7] loss: 0.043\n",
      "[Epoch 3, Batch 8] loss: 0.056\n",
      "[Epoch 3, Batch 9] loss: 0.058\n",
      "[Epoch 3, Batch 10] loss: 0.068\n",
      "[Epoch 3, Batch 11] loss: 0.068\n",
      "[Epoch 3, Batch 12] loss: 0.074\n",
      "[Epoch 3, Batch 13] loss: 0.078\n",
      "[Epoch 3, Batch 14] loss: 0.082\n",
      "[Epoch 3, Batch 15] loss: 0.096\n",
      "[Epoch 3, Batch 16] loss: 0.108\n",
      "[Epoch 3, Batch 17] loss: 0.113\n",
      "[Epoch 3, Batch 18] loss: 0.118\n",
      "[Epoch 3, Batch 19] loss: 0.129\n",
      "[Epoch 3, Batch 20] loss: 0.139\n",
      "[Epoch 3, Batch 21] loss: 0.140\n",
      "[Epoch 3, Batch 22] loss: 0.152\n",
      "[Epoch 3, Batch 23] loss: 0.156\n",
      "[Epoch 3, Batch 24] loss: 0.163\n",
      "[Epoch 3, Batch 25] loss: 0.182\n",
      "[Epoch 3, Batch 26] loss: 0.193\n",
      "[Epoch 3, Batch 27] loss: 0.209\n",
      "[Epoch 3, Batch 28] loss: 0.216\n",
      "[Epoch 3, Batch 29] loss: 0.224\n",
      "[Epoch 3, Batch 30] loss: 0.235\n",
      "[Epoch 3, Batch 31] loss: 0.249\n",
      "[Epoch 3, Batch 32] loss: 0.260\n",
      "[Epoch 3, Batch 33] loss: 0.263\n",
      "[Epoch 3, Batch 34] loss: 0.277\n",
      "[Epoch 3, Batch 35] loss: 0.280\n",
      "[Epoch 3, Batch 36] loss: 0.281\n",
      "[Epoch 3, Batch 37] loss: 0.292\n",
      "[Epoch 3, Batch 38] loss: 0.303\n",
      "[Epoch 3, Batch 39] loss: 0.312\n",
      "[Epoch 3, Batch 40] loss: 0.314\n",
      "[Epoch 3, Batch 41] loss: 0.314\n",
      "[Epoch 3, Batch 42] loss: 0.314\n",
      "[Epoch 3, Batch 43] loss: 0.327\n",
      "[Epoch 3, Batch 44] loss: 0.337\n",
      "[Epoch 3, Batch 45] loss: 0.354\n",
      "[Epoch 3, Batch 46] loss: 0.363\n",
      "[Epoch 3, Batch 47] loss: 0.370\n",
      "[Epoch 3, Batch 48] loss: 0.372\n",
      "[Epoch 3, Batch 49] loss: 0.381\n",
      "[Epoch 3, Batch 50] loss: 0.382\n",
      "[Epoch 3, Batch 51] loss: 0.389\n",
      "[Epoch 3, Batch 52] loss: 0.407\n",
      "[Epoch 3, Batch 53] loss: 0.424\n",
      "[Epoch 3, Batch 54] loss: 0.437\n",
      "[Epoch 3, Batch 55] loss: 0.441\n",
      "[Epoch 3, Batch 56] loss: 0.455\n",
      "[Epoch 3, Batch 57] loss: 0.474\n",
      "[Epoch 3, Batch 58] loss: 0.479\n",
      "[Epoch 3, Batch 59] loss: 0.499\n",
      "[Epoch 3, Batch 60] loss: 0.503\n",
      "[Epoch 3, Batch 61] loss: 0.516\n",
      "[Epoch 3, Batch 62] loss: 0.519\n",
      "[Epoch 3, Batch 63] loss: 0.525\n",
      "[Epoch 3, Batch 64] loss: 0.531\n",
      "[Epoch 3, Batch 65] loss: 0.549\n",
      "[Epoch 3, Batch 66] loss: 0.552\n",
      "[Epoch 3, Batch 67] loss: 0.563\n",
      "[Epoch 3, Batch 68] loss: 0.567\n",
      "[Epoch 3, Batch 69] loss: 0.570\n",
      "[Epoch 3, Batch 70] loss: 0.571\n",
      "[Epoch 3, Batch 71] loss: 0.578\n",
      "[Epoch 3, Batch 72] loss: 0.582\n",
      "[Epoch 3, Batch 73] loss: 0.582\n",
      "[Epoch 3, Batch 74] loss: 0.583\n",
      "[Epoch 3, Batch 75] loss: 0.587\n",
      "[Epoch 3, Batch 76] loss: 0.590\n",
      "[Epoch 3, Batch 77] loss: 0.600\n",
      "[Epoch 3, Batch 78] loss: 0.600\n",
      "[Epoch 3, Batch 79] loss: 0.618\n",
      "[Epoch 4, Batch 1] loss: 0.008\n",
      "[Epoch 4, Batch 2] loss: 0.010\n",
      "[Epoch 4, Batch 3] loss: 0.016\n",
      "[Epoch 4, Batch 4] loss: 0.016\n",
      "[Epoch 4, Batch 5] loss: 0.027\n",
      "[Epoch 4, Batch 6] loss: 0.035\n",
      "[Epoch 4, Batch 7] loss: 0.058\n",
      "[Epoch 4, Batch 8] loss: 0.068\n",
      "[Epoch 4, Batch 9] loss: 0.071\n",
      "[Epoch 4, Batch 10] loss: 0.084\n",
      "[Epoch 4, Batch 11] loss: 0.084\n",
      "[Epoch 4, Batch 12] loss: 0.090\n",
      "[Epoch 4, Batch 13] loss: 0.093\n",
      "[Epoch 4, Batch 14] loss: 0.097\n",
      "[Epoch 4, Batch 15] loss: 0.114\n",
      "[Epoch 4, Batch 16] loss: 0.128\n",
      "[Epoch 4, Batch 17] loss: 0.137\n",
      "[Epoch 4, Batch 18] loss: 0.142\n",
      "[Epoch 4, Batch 19] loss: 0.155\n",
      "[Epoch 4, Batch 20] loss: 0.167\n",
      "[Epoch 4, Batch 21] loss: 0.169\n",
      "[Epoch 4, Batch 22] loss: 0.185\n",
      "[Epoch 4, Batch 23] loss: 0.188\n",
      "[Epoch 4, Batch 24] loss: 0.196\n",
      "[Epoch 4, Batch 25] loss: 0.212\n",
      "[Epoch 4, Batch 26] loss: 0.222\n",
      "[Epoch 4, Batch 27] loss: 0.235\n",
      "[Epoch 4, Batch 28] loss: 0.244\n",
      "[Epoch 4, Batch 29] loss: 0.255\n",
      "[Epoch 4, Batch 30] loss: 0.268\n",
      "[Epoch 4, Batch 31] loss: 0.280\n",
      "[Epoch 4, Batch 32] loss: 0.290\n",
      "[Epoch 4, Batch 33] loss: 0.292\n",
      "[Epoch 4, Batch 34] loss: 0.307\n",
      "[Epoch 4, Batch 35] loss: 0.310\n",
      "[Epoch 4, Batch 36] loss: 0.311\n",
      "[Epoch 4, Batch 37] loss: 0.319\n",
      "[Epoch 4, Batch 38] loss: 0.326\n",
      "[Epoch 4, Batch 39] loss: 0.334\n",
      "[Epoch 4, Batch 40] loss: 0.335\n",
      "[Epoch 4, Batch 41] loss: 0.335\n",
      "[Epoch 4, Batch 42] loss: 0.335\n",
      "[Epoch 4, Batch 43] loss: 0.344\n",
      "[Epoch 4, Batch 44] loss: 0.352\n",
      "[Epoch 4, Batch 45] loss: 0.363\n",
      "[Epoch 4, Batch 46] loss: 0.369\n",
      "[Epoch 4, Batch 47] loss: 0.375\n",
      "[Epoch 4, Batch 48] loss: 0.376\n",
      "[Epoch 4, Batch 49] loss: 0.384\n",
      "[Epoch 4, Batch 50] loss: 0.384\n",
      "[Epoch 4, Batch 51] loss: 0.388\n",
      "[Epoch 4, Batch 52] loss: 0.399\n",
      "[Epoch 4, Batch 53] loss: 0.410\n",
      "[Epoch 4, Batch 54] loss: 0.422\n",
      "[Epoch 4, Batch 55] loss: 0.425\n",
      "[Epoch 4, Batch 56] loss: 0.441\n",
      "[Epoch 4, Batch 57] loss: 0.454\n",
      "[Epoch 4, Batch 58] loss: 0.457\n",
      "[Epoch 4, Batch 59] loss: 0.469\n",
      "[Epoch 4, Batch 60] loss: 0.472\n",
      "[Epoch 4, Batch 61] loss: 0.483\n",
      "[Epoch 4, Batch 62] loss: 0.486\n",
      "[Epoch 4, Batch 63] loss: 0.489\n",
      "[Epoch 4, Batch 64] loss: 0.493\n",
      "[Epoch 4, Batch 65] loss: 0.505\n",
      "[Epoch 4, Batch 66] loss: 0.507\n",
      "[Epoch 4, Batch 67] loss: 0.516\n",
      "[Epoch 4, Batch 68] loss: 0.519\n",
      "[Epoch 4, Batch 69] loss: 0.521\n",
      "[Epoch 4, Batch 70] loss: 0.522\n",
      "[Epoch 4, Batch 71] loss: 0.527\n",
      "[Epoch 4, Batch 72] loss: 0.531\n",
      "[Epoch 4, Batch 73] loss: 0.531\n",
      "[Epoch 4, Batch 74] loss: 0.532\n",
      "[Epoch 4, Batch 75] loss: 0.535\n",
      "[Epoch 4, Batch 76] loss: 0.538\n",
      "[Epoch 4, Batch 77] loss: 0.547\n",
      "[Epoch 4, Batch 78] loss: 0.547\n",
      "[Epoch 4, Batch 79] loss: 0.560\n",
      "[Epoch 5, Batch 1] loss: 0.008\n",
      "[Epoch 5, Batch 2] loss: 0.009\n",
      "[Epoch 5, Batch 3] loss: 0.013\n",
      "[Epoch 5, Batch 4] loss: 0.013\n",
      "[Epoch 5, Batch 5] loss: 0.021\n",
      "[Epoch 5, Batch 6] loss: 0.025\n",
      "[Epoch 5, Batch 7] loss: 0.034\n",
      "[Epoch 5, Batch 8] loss: 0.044\n",
      "[Epoch 5, Batch 9] loss: 0.047\n",
      "[Epoch 5, Batch 10] loss: 0.056\n",
      "[Epoch 5, Batch 11] loss: 0.057\n",
      "[Epoch 5, Batch 12] loss: 0.061\n",
      "[Epoch 5, Batch 13] loss: 0.063\n",
      "[Epoch 5, Batch 14] loss: 0.065\n",
      "[Epoch 5, Batch 15] loss: 0.073\n",
      "[Epoch 5, Batch 16] loss: 0.081\n",
      "[Epoch 5, Batch 17] loss: 0.085\n",
      "[Epoch 5, Batch 18] loss: 0.088\n",
      "[Epoch 5, Batch 19] loss: 0.094\n",
      "[Epoch 5, Batch 20] loss: 0.104\n",
      "[Epoch 5, Batch 21] loss: 0.105\n",
      "[Epoch 5, Batch 22] loss: 0.113\n",
      "[Epoch 5, Batch 23] loss: 0.114\n",
      "[Epoch 5, Batch 24] loss: 0.120\n",
      "[Epoch 5, Batch 25] loss: 0.131\n",
      "[Epoch 5, Batch 26] loss: 0.138\n",
      "[Epoch 5, Batch 27] loss: 0.148\n",
      "[Epoch 5, Batch 28] loss: 0.154\n",
      "[Epoch 5, Batch 29] loss: 0.162\n",
      "[Epoch 5, Batch 30] loss: 0.171\n",
      "[Epoch 5, Batch 31] loss: 0.180\n",
      "[Epoch 5, Batch 32] loss: 0.189\n",
      "[Epoch 5, Batch 33] loss: 0.191\n",
      "[Epoch 5, Batch 34] loss: 0.202\n",
      "[Epoch 5, Batch 35] loss: 0.205\n",
      "[Epoch 5, Batch 36] loss: 0.206\n",
      "[Epoch 5, Batch 37] loss: 0.218\n",
      "[Epoch 5, Batch 38] loss: 0.227\n",
      "[Epoch 5, Batch 39] loss: 0.234\n",
      "[Epoch 5, Batch 40] loss: 0.236\n",
      "[Epoch 5, Batch 41] loss: 0.236\n",
      "[Epoch 5, Batch 42] loss: 0.236\n",
      "[Epoch 5, Batch 43] loss: 0.252\n",
      "[Epoch 5, Batch 44] loss: 0.265\n",
      "[Epoch 5, Batch 45] loss: 0.286\n",
      "[Epoch 5, Batch 46] loss: 0.296\n",
      "[Epoch 5, Batch 47] loss: 0.304\n",
      "[Epoch 5, Batch 48] loss: 0.305\n",
      "[Epoch 5, Batch 49] loss: 0.313\n",
      "[Epoch 5, Batch 50] loss: 0.314\n",
      "[Epoch 5, Batch 51] loss: 0.318\n",
      "[Epoch 5, Batch 52] loss: 0.331\n",
      "[Epoch 5, Batch 53] loss: 0.343\n",
      "[Epoch 5, Batch 54] loss: 0.356\n",
      "[Epoch 5, Batch 55] loss: 0.359\n",
      "[Epoch 5, Batch 56] loss: 0.372\n",
      "[Epoch 5, Batch 57] loss: 0.385\n",
      "[Epoch 5, Batch 58] loss: 0.389\n",
      "[Epoch 5, Batch 59] loss: 0.397\n",
      "[Epoch 5, Batch 60] loss: 0.399\n",
      "[Epoch 5, Batch 61] loss: 0.414\n",
      "[Epoch 5, Batch 62] loss: 0.417\n",
      "[Epoch 5, Batch 63] loss: 0.422\n",
      "[Epoch 5, Batch 64] loss: 0.426\n",
      "[Epoch 5, Batch 65] loss: 0.440\n",
      "[Epoch 5, Batch 66] loss: 0.442\n",
      "[Epoch 5, Batch 67] loss: 0.449\n",
      "[Epoch 5, Batch 68] loss: 0.453\n",
      "[Epoch 5, Batch 69] loss: 0.456\n",
      "[Epoch 5, Batch 70] loss: 0.458\n",
      "[Epoch 5, Batch 71] loss: 0.466\n",
      "[Epoch 5, Batch 72] loss: 0.471\n",
      "[Epoch 5, Batch 73] loss: 0.471\n",
      "[Epoch 5, Batch 74] loss: 0.471\n",
      "[Epoch 5, Batch 75] loss: 0.475\n",
      "[Epoch 5, Batch 76] loss: 0.477\n",
      "[Epoch 5, Batch 77] loss: 0.484\n",
      "[Epoch 5, Batch 78] loss: 0.484\n",
      "[Epoch 5, Batch 79] loss: 0.497\n",
      "Accuracy of the network on the 10000 test images: 42.26 %\n",
      "[Epoch 1, Batch 79] loss: 0.499\n",
      "[Epoch 2, Batch 79] loss: 0.461\n",
      "[Epoch 3, Batch 79] loss: 0.473\n",
      "[Epoch 4, Batch 79] loss: 0.478\n",
      "[Epoch 5, Batch 79] loss: 0.420\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 54.34 %\n"
     ]
    }
   ],
   "source": [
    "net = net1\n",
    "la = la1\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "\n",
    "def lower_function(output, label, la):\n",
    "    crossentropy = nn.CrossEntropyLoss()\n",
    "    loss = crossentropy(output, label)*la\n",
    "    return loss\n",
    "\n",
    "# SGD的效果明显要比Adam好不少\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# 第一次测试网络\n",
    "test(net, testloader)\n",
    "\n",
    "# 训练网络\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = lower_function(outputs, labels, la[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\n",
    "    \n",
    "\n",
    "test(net, testloader)\n",
    "\n",
    "#la = la - 0.01 *  \n",
    "\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = lower_function(outputs, labels, la[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "test(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cad1066-1f46-473b-af25-cea5ca277ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90.8010)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(la.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f59d7-678c-40c0-80e8-50123f4b2a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
